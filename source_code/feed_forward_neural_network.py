# -*- coding: utf-8 -*-
"""Quiz3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EX2gw9CDBWv4IQ3SCRxY-x5nC4PbKMFd
"""

# Import the packages and print their versions
import tensorflow as tf
import keras
import scikeras

print(f"TensorFlow version: {tf.__version__}")
print(f"Keras version: {keras.__version__}")
print(f"SciKeras version: {scikeras.__version__}")

#!pip uninstall scikeras keras tensorflow -y

#!pip install tensorflow==2.15.0 keras==2.15.0 scikeras==0.11.0

import pandas as pd
import io
from google.colab import files
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import copy
from sklearn import datasets

"""# Upload Dataset"""

# Check if Churn_Modelling1.csv is already uploaded, if so, no need to run this cell
# skip to the next cell
# this cell just loads the file into the folders on the left hand side
uploaded = files.upload()

# loads the csv into a pandas dataframe, so we can clean it and transform it using pandas functions
df = pd.read_csv('Churn_Modelling1.csv')

# check to see the df is loaded properly
df

"""# Data Cleaning"""

# Drop columns that don't affect the target column
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# look if the columns have been dropped
df.columns

# encode the categorical features using the variable above
df = pd.get_dummies(df, columns=['Geography', 'Gender'], drop_first=True)
df

"""# Train, Test, Split


"""

# Split into train/test:
train, test = train_test_split(df, test_size=0.2, random_state=22)

"""# Scale features"""

# get all df.column names except Exited
features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Geography_Germany', 'Geography_Spain', 'Gender_Male']
features

# check the status of data frames with these features
# Exited is still in the train dataframe but won't show here because we've only printed the columns in all_features
train[features]

"""# Split features and target in train and test"""

#Split features and target in train and test
train_features = train[features]
train_target = train['Exited']
test_features = test[features]
test_target = test['Exited']

#check each dataset
train_features

train_target

test_features

test_target

"""# Feed-Forward Neural Network Model"""

# figure out number of columns for input for variable input_shape
train_features.shape

# define the feed-forward neural network model using Keras:
# we import Keras first
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, InputLayer

# def nnet_model():
#   model = Sequential()
#   model.add(Dense(12, input_shape=(11,), activation='relu'))
#   model.add(Dense(8, activation='relu'))
#   model.add(Dense(1, activation='sigmoid'))
#   return model

# this was the code suggested by Gemini after error from the above function
def create_model(number_of_layers=1, number_of_nodes=8, activation_function='relu'):
    model = Sequential()
    model.add(InputLayer(input_shape=(train_features.shape[1],)))  # Assuming you have defined train_features elsewhere
    for _ in range(number_of_layers):
        model.add(Dense(number_of_nodes, activation=activation_function))
    model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# import KeraClassifier and GridSearch
from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import GridSearchCV

# compile model
# the following code in this cell was the from module 8
# We've placed this inside the create_model function for readability, and to not repeat the code
# This was recommended by Gemini

# model = create_model()
# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# create KerasClassifer object, which is the model to use for GridSearch
model_opt = KerasClassifier(build_fn=create_model, verbose=0)

"""# GridSearch

"""

# Define the hyperparameter grid
# param_grid = {
#     'number_of_layers': [1, 2, 3],
#     'number_of_nodes': [8, 16, 32],
#     'activation_function': ['relu', 'sigmoid', 'tanh']
# }

# Define the hyperparameter grid
param_grid = {
    'model__number_of_layers': [2, 3, 4, 5],
    'model__number_of_nodes': [8, 16, 32],
    'model__activation_function': ['relu', 'sigmoid', 'tanh'],
    'optimizer__learning_rate': [0.001, 0.01, 0.1]
}

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=model_opt, param_grid=param_grid, cv=3, n_jobs=-1)

# Fit the GridSearchCV object to the training data
grid_result = grid_search.fit(train_features, train_target)

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

# Create the model with the best parameters
model_with_best_parameters = create_model(number_of_layers=2, number_of_nodes=8, activation_function='sigmoid')

# convert input dataset to correct data type for evalatuion
train_features_float = np.array(train_features, dtype=np.float32)
train_target_float = np.array(train_target, dtype=np.float32)
test_features_float = np.array(test_features, dtype=np.float32)
test_target_float = np.array(test_target, dtype=np.float32)

# evaluate the keras model on the train set
_, accuracy = model_with_best_parameters.evaluate(train_features_float, train_target_float, verbose=0)
print('Accuracy: %.2f' % (accuracy*100))

# predictions on the test set
predictions = model_with_best_parameters.predict(test_features_float)
test['prediction_opt'] = predictions

# evaluate the keras model on the test set:
_, accuracy = model_with_best_parameters.evaluate(test_features_float, test_target_float, verbose=0)
print('Accuracy: %.2f' % (accuracy*100))

# Plot the predictions
plt.hist(test.loc[test_target==1, 'prediction_opt'], bins=50, alpha=0.5, label='target true')
plt.hist(test.loc[test_target==0, 'prediction_opt'], bins=50, alpha=0.5, label='target false')
plt.legend(loc='upper right')
plt.show()